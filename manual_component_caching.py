#!/usr/bin/env python3
"""
Manual Component Caching via Direct Model Check
Cache components by triggering model downloads through existing handler
"""

import subprocess
import json
import os
import time

def trigger_component_downloads():
    """Trigger component downloads via model loading"""
    
    api_key = os.getenv("RUNPOD_API_KEY")
    if not api_key:
        print("‚ùå RUNPOD_API_KEY environment variable not set")
        return False
    
    endpoint_id = "zu0ik6c8yukyl6"
    
    print("=" * 80)
    print("MANUAL COMPONENT CACHING VIA MODEL LOADING")
    print("=" * 80)
    print("This will trigger downloads of missing components by loading models")
    print("Components will be cached to /runpod-volume/huggingface")
    print("=" * 80)
    
    # Create a job that forces model loading which will cache components
    cache_job = {
        "input": {
            "action": "generate",
            "audio_1": "test.wav",  # This will trigger model loading
            "condition_image": "test.png",
            "prompt": "test",
            "dry_run": True  # If supported, just load models without generating
        }
    }
    
    url = f"https://api.runpod.ai/v2/{endpoint_id}/run"
    
    curl_cmd = [
        "curl", "-X", "POST", 
        url,
        "-H", "Content-Type: application/json",
        "-H", f"Authorization: Bearer {api_key}",
        "-d", json.dumps(cache_job)
    ]
    
    try:
        result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            response = json.loads(result.stdout)
            job_id = response.get("id")
            
            if job_id:
                print(f"‚úÖ Model loading job submitted: {job_id}")
                print("‚è±Ô∏è  This will download and cache missing components")
                
                # Monitor for model loading (components will be cached)
                for i in range(300):  # 5 minutes
                    status_url = f"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}"
                    
                    status_cmd = [
                        "curl", "-H", f"Authorization: Bearer {api_key}",
                        status_url
                    ]
                    
                    status_result = subprocess.run(status_cmd, capture_output=True, text=True)
                    
                    if status_result.returncode == 0:
                        status_data = json.loads(status_result.stdout)
                        status = status_data.get("status")
                        
                        if i % 30 == 0:
                            print(f"[{i//30 * 30}s] Status: {status}")
                        
                        if status == "COMPLETED":
                            print(f"\nüì¶ Model loading completed!")
                            print("‚úÖ Components should now be cached in /runpod-volume/huggingface")
                            return True
                        
                        elif status == "FAILED":
                            output = status_data.get("output", {})
                            print(f"\n‚ö†Ô∏è  Job failed but may have cached components: {output}")
                            return True  # Even failed jobs may cache components
                        
                        elif status in ["IN_QUEUE", "IN_PROGRESS"]:
                            time.sleep(1)
                            continue
                        else:
                            print(f"‚ö†Ô∏è  Status: {status}")
                            time.sleep(1)
                            continue
                    else:
                        print(f"‚ùå Status check failed: {status_result.stderr}")
                        return False
                
                print("‚è±Ô∏è  Job monitoring timeout")
                return True  # May have cached components anyway
            else:
                print(f"‚ùå No job ID in response: {response}")
                return False
        else:
            print(f"‚ùå Request failed: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def check_huggingface_cache():
    """Check what's in the HuggingFace cache"""
    
    api_key = os.getenv("RUNPOD_API_KEY")
    if not api_key:
        return False
    
    endpoint_id = "zu0ik6c8yukyl6"
    
    # Check cache contents
    cache_check = {
        "input": {
            "action": "model_check"
        }
    }
    
    url = f"https://api.runpod.ai/v2/{endpoint_id}/run"
    
    curl_cmd = [
        "curl", "-X", "POST", 
        url,
        "-H", "Content-Type: application/json",
        "-H", f"Authorization: Bearer {api_key}",
        "-d", json.dumps(cache_check)
    ]
    
    try:
        result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            response = json.loads(result.stdout)
            job_id = response.get("id")
            
            if job_id:
                print(f"Cache check job: {job_id}")
                
                # Wait for completion
                for i in range(60):
                    status_url = f"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}"
                    
                    status_cmd = [
                        "curl", "-H", f"Authorization: Bearer {api_key}",
                        status_url
                    ]
                    
                    status_result = subprocess.run(status_cmd, capture_output=True, text=True)
                    
                    if status_result.returncode == 0:
                        status_data = json.loads(status_result.stdout)
                        status = status_data.get("status")
                        
                        if status == "COMPLETED":
                            output = status_data.get("output", {})
                            
                            if "output" in output:
                                model_info = output["output"].get("model_info", {})
                                print(f"\nüìä Model Check Results:")
                                print(f"  Network Volume: {model_info.get('network_volume_mounted', False)}")
                                print(f"  Models Directory: {model_info.get('models_directory_exists', False)}")
                                print(f"  CUDA Available: {model_info.get('cuda_available', False)}")
                                print(f"  Version: {output['output'].get('version', 'Unknown')}")
                                
                                return True
                            else:
                                print(f"Unexpected output: {output}")
                                return False
                        
                        elif status == "FAILED":
                            print(f"‚ùå Cache check failed: {status_data}")
                            return False
                        
                        elif status in ["IN_QUEUE", "IN_PROGRESS"]:
                            time.sleep(1)
                            continue
                        else:
                            print(f"‚ö†Ô∏è  Status: {status}")
                            time.sleep(1)
                            continue
                    else:
                        print(f"‚ùå Status check failed: {status_result.stderr}")
                        return False
                
                print("‚è±Ô∏è  Cache check timeout")
                return False
            else:
                print(f"‚ùå No job ID in response: {response}")
                return False
        else:
            print(f"‚ùå Request failed: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def main():
    print("Manual Component Caching")
    print("Triggers model loading to cache components to network storage")
    print("=" * 80)
    
    # Step 1: Check current cache
    print("\nüîç Step 1: Checking current cache status...")
    check_huggingface_cache()
    
    # Step 2: Trigger model loading to cache components
    print("\n‚è¨ Step 2: Triggering model loading to cache components...")
    cache_success = trigger_component_downloads()
    
    if cache_success:
        print(f"\n‚úÖ Component caching completed!")
        print(f"üì¶ Components cached to /runpod-volume/huggingface")
        print(f"üöÄ This will improve cold start times")
        print(f"üîí Better reliability with cached components")
        
        # Step 3: Verify cache after loading
        print(f"\nüîç Step 3: Verifying cache after loading...")
        check_huggingface_cache()
        
        print(f"\nüéØ SUCCESS: Component caching via model loading complete!")
        print(f"Next: Components will be available for faster loading")
    else:
        print(f"\n‚ö†Ô∏è  Component caching may have failed")
        print(f"Check endpoint logs for details")

if __name__ == "__main__":
    main()