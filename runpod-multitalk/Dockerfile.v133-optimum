# MultiTalk V133 - Add optimum dependency for quantization
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime AS base

# Set timezone and clean up in single layer
ENV DEBIAN_FRONTEND=noninteractive TZ=UTC
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone && \
    apt-get update && apt-get install -y --no-install-recommends git ffmpeg && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Stage 2: Install dependencies with careful ordering
FROM base AS deps

# First, install NumPy 1.26.4 and lock it
RUN pip uninstall -y numpy && \
    pip install --no-cache-dir "numpy==1.26.4" && \
    pip install --no-cache-dir "numba==0.59.1"

# Install CUDA 11.8 specific torch packages BEFORE other dependencies
RUN pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu118 \
    torch==2.1.0+cu118 \
    torchvision==0.16.0+cu118 \
    xformers==0.0.22.post7

# Install optimum with quanto for quantization support (compatible with PyTorch 2.1.0)
# Use older version that doesn't require float8 support
RUN pip install --no-cache-dir "optimum[quanto]==1.17.1"

# Install distvae without dependencies to avoid NumPy upgrade
RUN pip install --no-cache-dir --no-deps distvae

# Install xfuser without dependencies
RUN pip install --no-cache-dir --no-deps "xfuser>=0.4.1"

# Install all other dependencies
RUN pip install --no-cache-dir \
    "transformers>=4.49.0" \
    "diffusers>=0.31.0" \
    "accelerate>=1.1.1" \
    "safetensors>=0.3.1" \
    "einops>=0.8.0" \
    "huggingface-hub>=0.19.0" \
    "tokenizers>=0.20.3" \
    "sentencepiece>=0.1.99" \
    "beautifulsoup4>=4.12.3" \
    "yunchang>=0.6.0" \
    "pytest" \
    "opencv-python" \
    "imageio" \
    "imageio-ffmpeg" \
    "scikit-image>=0.21.0" \
    "librosa>=0.10.0" \
    "soundfile>=0.12.1" \
    pyloudnorm==0.1.1 \
    "opencv-python-headless>=4.9.0.80" \
    imageio==2.33.1 \
    imageio-ffmpeg==0.4.9 \
    boto3 \
    runpod==1.7.3 \
    Pillow==10.2.0 \
    moviepy==1.0.3 \
    easydict \
    omegaconf \
    tensorboardX \
    ftfy \
    timm \
    sentencepiece \
    peft \
    rotary-embedding-torch \
    scipy

# CRITICAL: Force reinstall NumPy 1.26.4 one more time to ensure it's not upgraded
RUN pip uninstall -y numpy && \
    pip install --no-cache-dir --no-deps "numpy==1.26.4"

# Verify NumPy version is correct
RUN python -c "import numpy; assert numpy.__version__.startswith('1.26'), f'Wrong NumPy version: {numpy.__version__}'"

# Clean up
RUN pip cache purge && rm -rf /tmp/* /var/tmp/* ~/.cache

# Stage 3: Final image
FROM deps AS final

# Set environment variables
ENV PYTHONPATH=/app/multitalk_official:/app:/runpod-volume/models \
    HF_HOME=/runpod-volume/huggingface \
    TRANSFORMERS_CACHE=/runpod-volume/huggingface \
    MODEL_PATH=/runpod-volume/models \
    PYTHONUNBUFFERED=1

WORKDIR /app

# Clone and clean up in single layer
RUN git clone https://github.com/MeiGen-AI/MultiTalk.git /app/multitalk_official && \
    rm -rf /app/multitalk_official/.git

# Copy files and update version
COPY multitalk_v75_0_json_input.py /app/
COPY handler_v122_s3_fix.py /app/handler.py
COPY s3_handler.py /app/
RUN sed -i 's/V12[0-9]/V133/g' /app/handler.py

# Final verification (skip GPU-dependent imports and optimum.quanto which may need GPU)
RUN python -c "import numpy; print(f'NumPy {numpy.__version__}')" && \
    python -c "import torch; print(f'PyTorch {torch.__version__}')" && \
    python -c "import numba; print(f'Numba {numba.__version__}')" && \
    python -c "import distvae; print('distvae imported successfully')" && \
    python -c "import optimum; print('optimum imported successfully')" && \
    echo "V133: Added optimum[quanto] 1.17.1 for quantization support"

CMD ["python", "-u", "/app/handler.py"]