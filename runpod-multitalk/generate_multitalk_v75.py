#!/usr/bin/env python
"""
MultiTalk Generation Script - V75.0 JSON Input Format
Accepts JSON input format for multi-speaker video generation
"""
import argparse
import json
import os
import sys
import torch
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description='Generate MultiTalk videos')
    
    # Core arguments matching the official interface
    parser.add_argument('--task', type=str, default='multitalk-14B',
                        choices=['t2v-14B', 't2v-1.3B', 'i2v-14B', 't2i-14B', 
                                'flf2v-14B', 'vace-1.3B', 'vace-14B', 'multitalk-14B'])
    parser.add_argument('--size', type=str, default='multitalk-480',
                        choices=['720*1280', '1280*720', '480*832', '832*480', 
                                '1024*1024', 'multitalk-480', 'multitalk-720'])
    parser.add_argument('--frame_num', type=int, default=81)
    parser.add_argument('--ckpt_dir', type=str, required=True)
    parser.add_argument('--wav2vec_dir', type=str, required=True)
    parser.add_argument('--save_file', type=str, required=True)
    parser.add_argument('--base_seed', type=int, default=42)
    parser.add_argument('--input_json', type=str, required=True)
    parser.add_argument('--mode', type=str, default='clip', choices=['clip', 'streaming'])
    parser.add_argument('--sample_steps', type=int, default=40)
    parser.add_argument('--sample_text_guide_scale', type=float, default=7.5)
    parser.add_argument('--sample_audio_guide_scale', type=float, default=3.5)
    parser.add_argument('--num_persistent_param_in_dit', type=int, default=0)
    parser.add_argument('--use_teacache', action='store_true')
    
    args = parser.parse_args()
    
    # Load input JSON
    with open(args.input_json, 'r') as f:
        input_data = json.load(f)
    
    print(f"ðŸŽ¬ Generating MultiTalk video with JSON input...")
    print(f"  Task: {args.task}")
    print(f"  Size: {args.size}")
    print(f"  Frames: {args.frame_num}")
    print(f"  Steps: {args.sample_steps}")
    print(f"  Output: {args.save_file}")
    
    # Process speakers from input JSON
    speakers = input_data.get("speakers", [])
    if not speakers:
        raise ValueError("No speakers found in input JSON")
    
    # Create a simple video (placeholder for now)
    import cv2
    import numpy as np
    
    speaker = speakers[0]
    image_path = speaker["condition_image"]
    audio_path = speaker["condition_audio"]
    
    # Read image
    img = cv2.imread(image_path)
    if img is None:
        raise ValueError(f"Could not load image: {image_path}")
    
    h, w = img.shape[:2]
    
    # Create video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    output_path = f"{args.save_file}.mp4"
    out = cv2.VideoWriter(output_path, fourcc, 25.0, (w, h))
    
    # Generate frames
    for i in range(args.frame_num):
        frame = img.copy()
        # Add frame counter
        cv2.putText(frame, f"V75.0 JSON - Frame {i+1}/{args.frame_num}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(frame, f"JSON Input Mode - {args.task}", 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        out.write(frame)
    
    out.release()
    print(f"âœ… Video generated: {output_path}")

if __name__ == "__main__":
    main()